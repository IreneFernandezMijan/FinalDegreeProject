# Import core libraries for data manipulation, machine learning, visualization, and cheminformatics
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# RDKit for molecule handling, descriptor calculation, and fingerprinting
from rdkit import Chem, DataStructs
from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors
from rdkit.Chem.MolStandardize import rdMolStandardize
from rdkit.Chem import AllChem
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator

# Scikit-learn for preprocessing, model evaluation, and cross-validation
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, roc_curve,
                             mean_squared_error, r2_score)

# TensorFlow/Keras for deep learning model creation and training
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2

# Others
import os
import random
import tensorflow as tf
import joblib

# Set random seed for reproducibility across NumPy, TensorFlow, and Python
def set_seed(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Enables deterministic operations when supported

# Configuration for Morgan fingerprinting
FP_SIZE = 2048
RADIUS = 2
RANDOM_STATE = 42
morgan_generator = GetMorganGenerator(radius=RADIUS, fpSize=FP_SIZE)


# Canonicalize SMILES (standardized string representation of molecules)
def standardize_smiles(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        return Chem.MolToSmiles(mol, canonical=True)
    except:
        return None

# Convert SMILES to a Morgan fingerprint (binary vector representing molecular substructure)
def smiles_to_fingerprint(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(FP_SIZE, dtype=np.float32)
    fp = morgan_generator.GetFingerprint(mol)
    arr = np.zeros((FP_SIZE,), dtype=np.float32)
    DataStructs.ConvertToNumpyArray(fp, arr)
    return arr

# Extract 8 physical-chemical descriptors per molecule
def extract_physchem_features(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(9)
    return np.array([
        Descriptors.MolWt(mol),
        Descriptors.MolMR(mol),
        Descriptors.TPSA(mol),
        Crippen.MolLogP(mol),
        Lipinski.NumRotatableBonds(mol),
        Lipinski.NumHDonors(mol),
        Lipinski.NumHAcceptors(mol),
        rdMolDescriptors.CalcNumAromaticRings(mol),
        0.0  # Reserved for custom or future features
    ])

# Combine fingerprints and descriptors into a single feature matrix and prepare target values
def prepare_dataset(df, task):
    df = df.dropna(subset=["SMILES"]).reset_index(drop=True)
    df["SMILES"] = df["SMILES"].apply(standardize_smiles)
    df = df.dropna(subset=["SMILES"]).reset_index(drop=True)

    # Feature extraction
    fps = np.array([smiles_to_fingerprint(s) for s in df["SMILES"]])
    desc_raw = np.array([extract_physchem_features(s) for s in df["SMILES"]])

    # Standardize descriptors
    scaler = StandardScaler()
    desc_scaled = scaler.fit_transform(desc_raw)

    # Combine features
    X = np.concatenate([fps, desc_scaled], axis=1)

    # Define target variable
    y = df["BBB+/BBB-"].map({"BBB+": 1, "BBB-": 0}).to_numpy() if task == "classification" else df["logBB"].to_numpy()

    return X, y, scaler  # Return features, targets, and fitted scaler


# Define a dense feedforward neural network with dropout and L2 regularization
def build_model(input_dim, task):
    model = Sequential([
        Input(shape=(input_dim,)),
        Dense(96, activation="relu", kernel_regularizer=l2(0.001)),
        Dropout(0.4),
        Dense(48, activation="relu", kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        Dense(1, activation="sigmoid" if task == "classification" else "linear")
    ])
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss="binary_crossentropy" if task == "classification" else "mse",
        metrics=["accuracy"] if task == "classification" else ["mse"]
    )
    return model

# Perform 5-fold cross-validation and evaluate the model using metrics appropriate to the task
def evaluate_model(X, y, task):
    folds = StratifiedKFold(5, shuffle=True, random_state=RANDOM_STATE) if task == "classification" else KFold(5, shuffle=True, random_state=RANDOM_STATE)
    metrics = []
    histories = []

    for fold_idx, (train_idx, val_idx) in enumerate(folds.split(X, y)):
        print(f"\nFold {fold_idx + 1}/5")

        model = build_model(X.shape[1], task)
        early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

        history = model.fit(
            X[train_idx], y[train_idx],
            validation_data=(X[val_idx], y[val_idx]),
            epochs=100,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )
        histories.append(history)
        y_pred = model.predict(X[val_idx]).flatten()

        # Evaluate performance
        if task == "classification":
            y_bin = (y_pred > 0.5).astype(int)
            metrics.append({
                "accuracy": accuracy_score(y[val_idx], y_bin),
                "precision": precision_score(y[val_idx], y_bin),
                "recall": recall_score(y[val_idx], y_bin),
                "f1": f1_score(y[val_idx], y_bin),
                "roc_auc": roc_auc_score(y[val_idx], y_pred)
            })

            # Plot ROC curve
            fpr, tpr, _ = roc_curve(y[val_idx], y_pred)
            plt.figure(figsize=(6, 5))
            plt.plot(fpr, tpr, label=f'ROC Fold {fold_idx + 1}')
            plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
            plt.xlabel("FPR")
            plt.ylabel("TPR")
            plt.title(f"ROC Curve - Fold {fold_idx + 1}")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.show()

        else:
            metrics.append({
                "rmse": np.sqrt(mean_squared_error(y[val_idx], y_pred)),
                "r2": r2_score(y[val_idx], y_pred)
            })

            # Scatter plot of predictions vs true values
            plt.figure(figsize=(6, 5))
            plt.scatter(y[val_idx], y_pred, alpha=0.6)
            plt.xlabel("True")
            plt.ylabel("Predicted")
            plt.title(f"Regression: True vs Predicted - Fold {fold_idx + 1}")
            plt.grid(True)
            plt.tight_layout()
            plt.show()


        if task == 'classification':
        # Plot training and validation loss and accuracy
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        for h in histories:
            plt.plot(h.history['loss'], color='blue', alpha=0.3)
            plt.plot(h.history['val_loss'], color='orange', alpha=0.3)
        plt.title("Loss Curve (Classification)")
        plt.xlabel("Epochs")
        plt.ylabel("Loss")
        plt.legend(["Train Loss", "Val Loss"])

        plt.subplot(1, 2, 2)
        for h in histories:
            plt.plot(h.history['accuracy'], color='blue', alpha=0.3)
            plt.plot(h.history['val_accuracy'], color='orange', alpha=0.3)
        plt.title("Accuracy Curve (Classification)")
        plt.xlabel("Epochs")
        plt.ylabel("Accuracy")
        plt.legend(["Train Acc", "Val Acc"])

        plt.tight_layout()
        plt.show()

    else:
        # Plot MSE for regression
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        for h in histories:
            plt.plot(h.history['mse'], color='blue', alpha=0.3)
            plt.plot(h.history['val_mse'], color='orange', alpha=0.3)
        plt.title("Mean Squared Error (Regression)")
        plt.xlabel("Epochs")
        plt.ylabel("MSE")
        plt.legend(["Train MSE", "Val MSE"])
        plt.tight_layout()
        plt.show()

    return pd.DataFrame(metrics)

if __name__ == "__main__":
  set_seed(RANDOM_STATE)

  # Load datasets (classification and regression)
  df_class = pd.read_csv("https://raw.githubusercontent.com/theochem/B3DB/refs/heads/main/B3DB/B3DB_classification.tsv", sep="\t")
  df_regr = pd.read_csv("https://raw.githubusercontent.com/theochem/B3DB/refs/heads/main/B3DB/B3DB_regression.tsv", sep="\t")

  print("\n=== CLASSIFICATION ===")
  Xc, yc, _ = prepare_dataset(df_class, "classification")
  res_c = evaluate_model(Xc, yc, "classification")
  print("Classification Mean:\n", res_c.mean())
  print("Standard Deviation:\n", res_c.std())

  print("\n=== REGRESSION ===")
  Xr, yr, _ = prepare_dataset(df_regr, "regression")
  res_r = evaluate_model(Xr, yr, "regression")
  print("Regression Mean:\n", res_r.mean())
  print("Standard Deviation:\n", res_r.std())

  # Train final models and save them
  Xc_final, yc_final, scaler_classification_features = prepare_dataset(df_class, "classification")
  model_classification_final = build_model(Xc_final.shape[1], "classification")
  model_classification_final.fit(Xc_final, yc_final, epochs=100, batch_size=32, verbose=0,
                                 callbacks=[EarlyStopping(monitor="loss", patience=10, restore_best_weights=True)])
  model_classification_final.save('classification_model.keras')

  Xr_fps = np.array([smiles_to_fingerprint(s) for s in df_regr["SMILES"].dropna().apply(standardize_smiles)])
  Xr_desc_raw = np.array([extract_physchem_features(s) for s in df_regr["SMILES"].dropna().apply(standardize_smiles)])
  Xr_desc_scaled = scaler_classification_features.transform(Xr_desc_raw)
  Xr_final = np.concatenate([Xr_fps, Xr_desc_scaled], axis=1)
  yr_final = df_regr["logBB"].dropna().to_numpy()

  model_regression_final = build_model(Xr_final.shape[1], "regression")
  model_regression_final.fit(Xr_final, yr_final, epochs=100, batch_size=32, verbose=0,
                             callbacks=[EarlyStopping(monitor="loss", patience=10, restore_best_weights=True)])
  model_regression_final.save('regression_model.keras')

  # Save the scaler for later use
  joblib.dump(scaler_classification_features, 'scaler_physchem.pkl')

  print("Models and scaler saved successfully.")
