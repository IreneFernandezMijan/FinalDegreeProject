pip install pandas scikit-learn tensorflow keras rdkit matplotlib seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from rdkit import Chem, DataStructs
from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors
from rdkit.Chem.MolStandardize import rdMolStandardize
from rdkit.Chem import AllChem
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, roc_curve,
                             mean_squared_error, r2_score)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator
import os
import random
import tensorflow as tf
from tensorflow.keras.regularizers import l2
import joblib

def set_seed(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Opcional, solo en CPU/GPU compatibles

FP_SIZE = 2048
RADIUS = 2
RANDOM_STATE = 42
morgan_generator = GetMorganGenerator(radius=RADIUS, fpSize=FP_SIZE)

def standardize_smiles(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        return Chem.MolToSmiles(mol, canonical=True)
    except:
        return None

def smiles_to_fingerprint(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(FP_SIZE, dtype=np.float32)
    fp = morgan_generator.GetFingerprint(mol)
    arr = np.zeros((FP_SIZE,), dtype=np.float32)
    DataStructs.ConvertToNumpyArray(fp, arr)
    return arr

def extract_physchem_features(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(9)
    return np.array([
        Descriptors.MolWt(mol),
        Descriptors.MolMR(mol),
        Descriptors.TPSA(mol),
        Crippen.MolLogP(mol),
        Lipinski.NumRotatableBonds(mol),
        Lipinski.NumHDonors(mol),
        Lipinski.NumHAcceptors(mol),
        rdMolDescriptors.CalcNumAromaticRings(mol),
        0.0  # Placeholder
    ])

def prepare_dataset(df, task):
    df = df.dropna(subset=["SMILES"]).reset_index(drop=True)
    df["SMILES"] = df["SMILES"].apply(standardize_smiles)
    df = df.dropna(subset=["SMILES"]).reset_index(drop=True)

    fps = np.array([smiles_to_fingerprint(s) for s in df["SMILES"]])
    desc_raw = np.array([extract_physchem_features(s) for s in df["SMILES"]])

    # This is where the StandardScaler is created and fitted
    scaler = StandardScaler()
    desc_scaled = scaler.fit_transform(desc_raw)

    X = np.concatenate([fps, desc_scaled], axis=1)
    y = df["BBB+/BBB-"].map({"BBB+": 1, "BBB-": 0}).to_numpy() if task == "classification" else df["logBB"].to_numpy()

    # *** CRITICAL CHANGE: Return the scaler as the third value ***
    return X, y, scaler

def build_model(input_dim, task):
    model = Sequential([
        Input(shape=(input_dim,)),
        Dense(96, activation="relu", kernel_regularizer=l2(0.001)),
        Dropout(0.4),
        Dense(48, activation="relu", kernel_regularizer=l2(0.001)),
        Dropout(0.3),
        Dense(1, activation="sigmoid" if task == "classification" else "linear")
    ])
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss="binary_crossentropy" if task == "classification" else "mse",
        metrics=["accuracy"] if task == "classification" else ["mse"]
    )
    return model

def evaluate_model(X, y, task):
    folds = StratifiedKFold(5, shuffle=True, random_state=RANDOM_STATE) if task == "classification" else KFold(5, shuffle=True, random_state=RANDOM_STATE)
    metrics = []
    histories = []

    for fold_idx, (train_idx, val_idx) in enumerate(folds.split(X, y)):
        print(f"\nFold {fold_idx + 1}/5")

        model = build_model(X.shape[1], task)
        early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

        history = model.fit(
            X[train_idx], y[train_idx],
            validation_data=(X[val_idx], y[val_idx]),
            epochs=100,
            batch_size=32,
            callbacks=[early_stop],
            verbose=0
        )
        histories.append(history)

        y_pred = model.predict(X[val_idx]).flatten()

        if task == "classification":
            y_bin = (y_pred > 0.5).astype(int)
            metrics.append({
                "accuracy": accuracy_score(y[val_idx], y_bin),
                "precision": precision_score(y[val_idx], y_bin),
                "recall": recall_score(y[val_idx], y_bin),
                "f1": f1_score(y[val_idx], y_bin),
                "roc_auc": roc_auc_score(y[val_idx], y_pred)
            })

            fpr, tpr, _ = roc_curve(y[val_idx], y_pred)
            plt.figure(figsize=(6, 5))
            plt.plot(fpr, tpr, label=f'ROC Fold {fold_idx + 1}')
            plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
            plt.xlabel("FPR")
            plt.ylabel("TPR")
            plt.title(f"Curva ROC - Fold {fold_idx + 1}")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.show()

        else:
            metrics.append({
                "rmse": np.sqrt(mean_squared_error(y[val_idx], y_pred)),
                "r2": r2_score(y[val_idx], y_pred)
            })

            plt.figure(figsize=(6, 5))
            plt.scatter(y[val_idx], y_pred, alpha=0.6)
            plt.xlabel("Real")
            plt.ylabel("Predicho")
            plt.title(f"Regresión: valores reales vs predichos - Fold {fold_idx + 1}")
            plt.grid(True)
            plt.tight_layout()
            plt.show()

    if task == 'classification':
        # Curva de pérdida y exactitud generales
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        for h in histories:
            plt.plot(h.history['loss'], color='blue', alpha=0.3)
            plt.plot(h.history['val_loss'], color='orange', alpha=0.3)
        plt.title("Curva de pérdida (clasificación)")
        plt.xlabel("Épocas")
        plt.ylabel("Pérdida")
        plt.legend(["Train Loss", "Val Loss"])

        plt.subplot(1, 2, 2)
        for h in histories:
            plt.plot(h.history['accuracy'], color='blue', alpha=0.3)
            plt.plot(h.history['val_accuracy'], color='orange', alpha=0.3)
        plt.title("Curva de exactitud (clasificación)")
        plt.xlabel("Épocas")
        plt.ylabel("Exactitud")
        plt.legend(["Train Acc", "Val Acc"])

        plt.tight_layout()
        plt.show()

    else:
        # MSE de entrenamiento y validación
        plt.figure(figsize=(14, 5))

        plt.subplot(1, 2, 1)
        for h in histories:
            plt.plot(h.history['mse'], color='blue', alpha=0.3)
            plt.plot(h.history['val_mse'], color='orange', alpha=0.3)
        plt.title("Curva de error cuadrático medio (regresión)")
        plt.xlabel("Épocas")
        plt.ylabel("MSE")
        plt.legend(["Train MSE", "Val MSE"])
        plt.tight_layout()
        plt.show()

    return pd.DataFrame(metrics)

if __name__ == "__main__":
  set_seed(RANDOM_STATE)

  df_class = pd.read_csv("https://raw.githubusercontent.com/theochem/B3DB/refs/heads/main/B3DB/B3DB_classification.tsv", sep="\t")
  df_regr = pd.read_csv("https://raw.githubusercontent.com/theochem/B3DB/refs/heads/main/B3DB/B3DB_regression.tsv", sep="\t")

  print("\n=== CLASIFICACIÓN ===")
  # ANTES: Xc, yc = prepare_dataset(df_class, "classification")
  # AHORA: Esperamos 3 valores, pero el tercero lo ignoramos con _
  Xc, yc, _ = prepare_dataset(df_class, "classification")
  res_c = evaluate_model(Xc, yc, "classification")
  print("Media clasificación:\n", res_c.mean())
  print("Desviación:\n", res_c.std())

  print("\n=== REGRESIÓN ===")
  # ANTES: Xr, yr = prepare_dataset(df_regr, "regression")
  # AHORA: Esperamos 3 valores, pero el tercero lo ignoramos con _
  Xr, yr, _ = prepare_dataset(df_regr, "regression")
  res_r = evaluate_model(Xr, yr, "regression")
  print("Media regresión:\n", res_r.mean())
  print("Desviación:\n", res_r.std())

  Xc_final, yc_final, scaler_classification_features = prepare_dataset(df_class, "classification")
  model_classification_final = build_model(Xc_final.shape[1], "classification")
  model_classification_final.fit(Xc_final, yc_final, epochs=100, batch_size=32, verbose=0,
                                 callbacks=[EarlyStopping(monitor="loss", patience=10, restore_best_weights=True)])
  model_classification_final.save('classification_model.keras')

  Xr_fps = np.array([smiles_to_fingerprint(s) for s in df_regr["SMILES"].dropna().apply(standardize_smiles)])
  Xr_desc_raw = np.array([extract_physchem_features(s) for s in df_regr["SMILES"].dropna().apply(standardize_smiles)])
  Xr_desc_scaled = scaler_classification_features.transform(Xr_desc_raw) # Usamos el scaler de clasificación
  Xr_final = np.concatenate([Xr_fps, Xr_desc_scaled], axis=1)
  yr_final = df_regr["logBB"].dropna().to_numpy()

  model_regression_final = build_model(Xr_final.shape[1], "regression")
  model_regression_final.fit(Xr_final, yr_final, epochs=100, batch_size=32, verbose=0,
                             callbacks=[EarlyStopping(monitor="loss", patience=10, restore_best_weights=True)])
  model_regression_final.save('regression_model.keras')

  joblib.dump(scaler_classification_features, 'scaler_physchem.pkl')

  print("Modelos y scaler guardados exitosamente.")

